<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="生成对抗网络," />










<meta name="description" content="First paper[Generative Adversarial Nets] [Paper][Code](the First paper of GAN) Unclassified[Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks] [Paper][Code] [Adversarial A">
<meta name="keywords" content="生成对抗网络">
<meta property="og:type" content="article">
<meta property="og:title" content="GAN paper">
<meta property="og:url" content="http://yoursite.com/2018/10/24/GAN_paper/index.html">
<meta property="og:site_name" content="Wxia">
<meta property="og:description" content="First paper[Generative Adversarial Nets] [Paper][Code](the First paper of GAN) Unclassified[Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks] [Paper][Code] [Adversarial A">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-10-24T11:41:29.337Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GAN paper">
<meta name="twitter:description" content="First paper[Generative Adversarial Nets] [Paper][Code](the First paper of GAN) Unclassified[Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks] [Paper][Code] [Adversarial A">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/24/GAN_paper/"/>





  <title>GAN paper | Wxia</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Wxia</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/24/GAN_paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wangxiaxia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wxia">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">GAN paper</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-24T18:44:35+08:00">
                2018-10-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-GAN/" itemprop="url" rel="index">
                    <span itemprop="name">paper/GAN</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="First-paper"><a href="#First-paper" class="headerlink" title="First paper"></a>First paper</h2><p>[Generative Adversarial Nets] <a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">[Paper]</a><br><a href="https://github.com/goodfeli/adversarial" target="_blank" rel="noopener">[Code]</a>(the First paper of GAN)</p>
<h2 id="Unclassified"><a href="#Unclassified" class="headerlink" title="Unclassified"></a>Unclassified</h2><p>[Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks] <a href="https://arxiv.org/abs/1506.05751" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/facebook/eyescream" target="_blank" rel="noopener">[Code]</a></p>
<p>[Adversarial Autoencoders] <a href="http://arxiv.org/abs/1511.05644" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/musyoku/adversarial-autoencoder" target="_blank" rel="noopener">[Code]</a></p>
<p>[Generating Images with Perceptual Similarity Metrics based on Deep Networks] <a href="https://arxiv.org/pdf/1602.02644v2.pdf" target="_blank" rel="noopener">[Paper]</a></p>
<p>[Generating images with recurrent adversarial networks] <a href="https://arxiv.org/abs/1602.05110" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/ofirnachum/sequence_gan" target="_blank" rel="noopener">[Code]</a></p>
<p>[Generative Visual Manipulation on the Natural Image Manifold] <a href="https://people.eecs.berkeley.edu/~junyanz/projects/gvm/eccv16_gvm.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/junyanz/iGAN" target="_blank" rel="noopener">[Code]</a></p>
<p>[Learning What and Where to Draw] <a href="http://www.scottreed.info/files/nips2016.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/reedscot/nips2016" target="_blank" rel="noopener">[Code]</a></p>
<p>[Adversarial Training for Sketch Retrieval] <a href="http://link.springer.com/chapter/10.1007/978-3-319-46604-0_55" target="_blank" rel="noopener">[Paper]</a></p>
<p>[Generative Image Modeling using Style and Structure Adversarial Networks] <a href="https://arxiv.org/pdf/1603.05631.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/xiaolonw/ss-gan" target="_blank" rel="noopener">[Code]</a></p>
<p>[Generative Adversarial Networks as Variational Training of Energy Based Models] <a href="http://www.mathpubs.com/detail/1611.01799v1/Generative-Adversarial-Networks-as-Variational-Training-of-Energy-Based-Models" target="_blank" rel="noopener">[Paper]</a>(ICLR 2017)</p>
<p>[Synthesizing the preferred inputs for neurons in neural networks via deep generator networks] <a href="https://arxiv.org/pdf/1605.09304v5.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/Evolving-AI-Lab/synthesizing" target="_blank" rel="noopener">[Code]</a></p>
<p>[SalGAN: Visual Saliency Prediction with Generative Adversarial Networks] <a href="https://arxiv.org/abs/1701.01081" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/imatge-upc/saliency-salgan-2017" target="_blank" rel="noopener">[Code]</a></p>
<p>[Adversarial Feature Learning] <a href="https://arxiv.org/abs/1605.09782" target="_blank" rel="noopener">[Paper]</a></p>
<p>[Adversarially Learned Inference]<a href="https://arxiv.org/abs/1606.00704" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/IshmaelBelghazi/ALI" target="_blank" rel="noopener">[Code]</a></p>
<h2 id="GAN-Theory"><a href="#GAN-Theory" class="headerlink" title="GAN Theory"></a>GAN Theory</h2><p>[Energy-based generative adversarial network] <a href="https://arxiv.org/pdf/1609.03126v2.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/buriburisuri/ebgan" target="_blank" rel="noopener">[Code]</a>(Lecun paper)</p>
<p>[Improved Techniques for Training GANs] <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/openai/improved-gan" target="_blank" rel="noopener">[Code]</a>(Goodfellow’s paper)</p>
<p>[Mode Regularized Generative Adversarial Networks] <a href="https://openreview.net/pdf?id=HJKkY35le" target="_blank" rel="noopener">[Paper]</a>(Yoshua Bengio , ICLR 2017)</p>
<p>[Improving Generative Adversarial Networks with Denoising Feature Matching] <a href="https://openreview.net/pdf?id=S1X7nhsxl" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/hvy/chainer-gan-denoising-feature-matching" target="_blank" rel="noopener">[Code]</a>(Yoshua Bengio , ICLR 2017)</p>
<p>[Sampling Generative Networks] <a href="https://arxiv.org/abs/1609.04468" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/dribnet/plat" target="_blank" rel="noopener">[Code]</a></p>
<p>[How to train Gans] <a href="https://github.com/soumith/ganhacks#authors" target="_blank" rel="noopener">[Docu]</a></p>
<p>[Towards Principled Methods for Training Generative Adversarial Networks] <a href="http://openreview.net/forum?id=Hk4_qw5xe" target="_blank" rel="noopener">[Paper]</a>(ICLR 2017)</p>
<p>[Unrolled Generative Adversarial Networks] <a href="https://arxiv.org/abs/1611.02163" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/poolio/unrolled_gan" target="_blank" rel="noopener">[Code]</a>(ICLR 2017)</p>
<p>[Least Squares Generative Adversarial Networks] <a href="https://arxiv.org/abs/1611.04076" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/pfnet-research/chainer-LSGAN" target="_blank" rel="noopener">[Code]</a>(ICCV 2017)</p>
<p>[Wasserstein GAN] <a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/martinarjovsky/WassersteinGAN" target="_blank" rel="noopener">[Code]</a></p>
<p>[Improved Training of Wasserstein GANs] <a href="https://arxiv.org/abs/1704.00028" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/igul222/improved_wgan_training" target="_blank" rel="noopener">[Code]</a>(The improve of wgan)</p>
<p>[Towards Principled Methods for Training Generative Adversarial Networks] <a href="https://arxiv.org/abs/1701.04862" target="_blank" rel="noopener">[Paper]</a></p>
<p>[Generalization and Equilibrium in Generative Adversarial Nets] <a href="https://arxiv.org/abs/1703.00573" target="_blank" rel="noopener">[Paper]</a>（ICML 2017）</p>
<p>[GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium]<a href="https://arxiv.org/abs/1706.08500" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/bioinf-jku/TTUR" target="_blank" rel="noopener">[code]</a></p>
<p>[Spectral Normalization for Generative Adversarial Networks]<a href="https://openreview.net/forum?id=B1QRgziT-" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/minhnhat93/tf-SNDCGAN" target="_blank" rel="noopener">[code]</a>（ICLR 2018）</p>
<p>[Which Training Methods for GANs do actually Converge]<a href="https://arxiv.org/pdf/1801.04406.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/LMescheder/GAN_stability" target="_blank" rel="noopener">[code]</a>（ICML 2018）</p>
<h2 id="Generation-High-Quality-Images"><a href="#Generation-High-Quality-Images" class="headerlink" title="Generation High-Quality Images"></a>Generation High-Quality Images</h2><p>[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks] <a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/jacobgil/keras-dcgan" target="_blank" rel="noopener">[Code]</a>(Gan with convolutional networks)(ICLR)</p>
<p>[Generative Adversarial Text to Image Synthesis] <a href="https://arxiv.org/abs/1605.05396" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/reedscot/icml2016" target="_blank" rel="noopener">[Code]</a><a href="https://github.com/paarthneekhara/text-to-image" target="_blank" rel="noopener">[code]</a></p>
<p>[Improved Techniques for Training GANs] <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/openai/improved-gan" target="_blank" rel="noopener">[Code]</a>(Goodfellow’s paper)</p>
<p>[Plug &amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space] <a href="https://arxiv.org/abs/1612.00005v1" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/Evolving-AI-Lab/ppgn" target="_blank" rel="noopener">[Code]</a></p>
<p>[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks] <a href="https://arxiv.org/pdf/1612.03242v1.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/hanzhanggit/StackGAN" target="_blank" rel="noopener">[Code]</a></p>
<p>[Improved Training of Wasserstein GANs] <a href="https://arxiv.org/abs/1704.00028" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/igul222/improved_wgan_training" target="_blank" rel="noopener">[Code]</a></p>
<p>[Boundary Equibilibrium Generative Adversarial Networks Implementation in Tensorflow] <a href="https://arxiv.org/abs/1703.10717" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/artcg/BEGAN" target="_blank" rel="noopener">[Code]</a></p>
<p>[Progressive Growing of GANs for Improved Quality, Stability, and Variation] <a href="http://research.nvidia.com/publication/2017-10_Progressive-Growing-of" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/tkarras/progressive_growing_of_gans" target="_blank" rel="noopener">[Code]</a><a href="https://github.com/zhangqianhui/PGGAN-tensorflow" target="_blank" rel="noopener">[Tensorflow Code]</a></p>
<p>[ Self-Attention Generative Adversarial Networks ] <a href="https://arxiv.org/abs/1805.08318" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/heykeetae/Self-Attention-GAN" target="_blank" rel="noopener">[Code]</a>(NIPS 2018)</p>
<p>[Large Scale GAN Training for High Fidelity Natural Image Synthesis] <a href="https://arxiv.org/abs/1809.11096" target="_blank" rel="noopener">[Paper]</a>(smbmitted to ICLR 2019)</p>
<h2 id="Semi-Supervised-Learning"><a href="#Semi-Supervised-Learning" class="headerlink" title="Semi-Supervised Learning"></a>Semi-Supervised Learning</h2><p>[Adversarial Training Methods for Semi-Supervised Text Classification] <a href="https://arxiv.org/abs/1605.07725" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/adversarial-text-classification.md" target="_blank" rel="noopener">[Note]</a>( Ian Goodfellow Paper)</p>
<p>[Improved Techniques for Training GANs] <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/openai/improved-gan" target="_blank" rel="noopener">[Code]</a>(Goodfellow’s paper)</p>
<p>[Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks] <a href="https://arxiv.org/abs/1511.06390" target="_blank" rel="noopener">[Paper]</a>(ICLR)</p>
<p>[Semi-Supervised QA with Generative Domain-Adaptive Nets] <a href="https://arxiv.org/abs/1702.02206" target="_blank" rel="noopener">[Paper]</a>(ACL 2017)</p>
<p>[Good Semi-supervised Learning that Requires a Bad GAN] <a href="https://arxiv.org/abs/1705.09783" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/kimiyoung/ssl_bad_gan" target="_blank" rel="noopener">[Code]</a>(NIPS 2017)</p>
<h2 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h2><p>[AdaGAN: Boosting Generative Models] <a href="https://arxiv.org/abs/1701.02386" target="_blank" rel="noopener">[Paper]</a>[[Code]]（Google Brain）</p>
<h2 id="Image-blending"><a href="#Image-blending" class="headerlink" title="Image blending"></a>Image blending</h2><p>[GP-GAN: Towards Realistic High-Resolution Image Blending] <a href="https://arxiv.org/abs/1703.07195" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/wuhuikai/GP-GAN" target="_blank" rel="noopener">[Code]</a></p>
<h2 id="Image-Inpainting"><a href="#Image-Inpainting" class="headerlink" title="Image Inpainting"></a>Image Inpainting</h2><p>[Semantic Image Inpainting with Perceptual and Contextual Losses] <a href="https://arxiv.org/abs/1607.07539" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="noopener">[Code]</a>(CVPR 2017)</p>
<p>[Context Encoders: Feature Learning by Inpainting] <a href="https://arxiv.org/abs/1604.07379" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/jazzsaxmafia/Inpainting" target="_blank" rel="noopener">[Code]</a></p>
<p>[Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks] <a href="https://arxiv.org/abs/1611.06430v1" target="_blank" rel="noopener">[Paper]</a></p>
<p>[Generative face completion] <a href="https://drive.google.com/file/d/0B8_MZ8a8aoSeenVrYkpCdnFRVms/edit" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/Yijunmaverick/GenerativeFaceCompletion" target="_blank" rel="noopener">[code]</a>(CVPR2017)</p>
<p>[Globally and Locally Consistent Image Completion] <a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/" target="_blank" rel="noopener">[MainPAGE]</a><a href="https://github.com/satoshiiizuka/siggraph2017_inpainting" target="_blank" rel="noopener">[code]</a>(SIGGRAPH 2017)</p>
<p>[High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis] <a href="https://arxiv.org/abs/1611.09969" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/leehomyc/Faster-High-Res-Neural-Inpainting" target="_blank" rel="noopener">[code]</a>(CVPR 2017)</p>
<p>[Eye In-Painting with Exemplar Generative Adversarial Networks] <a href="https://arxiv.org/abs/1712.03999" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/bdol/exemplar_gans" target="_blank" rel="noopener">[Introduction]</a><a href="https://github.com/zhangqianhui/Exemplar_GAN_Eye_Inpainting" target="_blank" rel="noopener">[Tensorflow code]</a>(CVPR2018)</p>
<p>[Generative Image Inpainting with Contextual Attention] <a href="https://arxiv.org/abs/1801.07892" target="_blank" rel="noopener">[Paper]</a><a href="http://jiahuiyu.com/deepfill" target="_blank" rel="noopener">[Project]</a><a href="http://jiahuiyu.com/deepfill" target="_blank" rel="noopener">[Demo]</a><a href="https://youtu.be/xz1ZvcdhgQ0" target="_blank" rel="noopener">[YouTube]</a><a href="https://github.com/JiahuiYu/generative_inpainting" target="_blank" rel="noopener">[Code]</a>(CVPR2018)</p>
<p>[Free-Form Image Inpainting with Gated Convolution] <a href="https://arxiv.org/abs/1806.03589" target="_blank" rel="noopener">[Paper]</a><a href="http://jiahuiyu.com/deepfill2" target="_blank" rel="noopener">[Project]</a><a href="https://youtu.be/uZkEi9Y2dj4" target="_blank" rel="noopener">[YouTube]</a></p>
<h2 id="Re-identification"><a href="#Re-identification" class="headerlink" title="Re-identification"></a>Re-identification</h2><p>[Pose-Normalized Image Generation for Person Re-identification] <a href="https://arxiv.org/abs/1712.02225" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/naiq/PN_GAN" target="_blank" rel="noopener">[Code]</a>(ECCV 2018)</p>
<h2 id="Super-Resolution"><a href="#Super-Resolution" class="headerlink" title="Super-Resolution"></a>Super-Resolution</h2><p>[Image super-resolution through deep learning ]<a href="https://github.com/david-gpu/srez" target="_blank" rel="noopener">[Code]</a>(Just for face dataset)</p>
<p>[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network] <a href="https://arxiv.org/abs/1609.04802" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/leehomyc/Photo-Realistic-Super-Resoluton" target="_blank" rel="noopener">[Code]</a>（Using Deep residual network）</p>
<p>[EnhanceGAN] <a href="https://medium.com/@richardherbert/faces-from-noise-super-enhancing-8x8-images-with-enhancegan-ebda015bb5e0#.io6pskvin" target="_blank" rel="noopener">[Docs]</a>[[Code]]</p>
<p>[ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks] <a href="https://arxiv.org/abs/1809.00219" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/xinntao/ESRGAN" target="_blank" rel="noopener">[Code]</a>(ECCV 2018 workshop)</p>
<h2 id="De-Occlusion"><a href="#De-Occlusion" class="headerlink" title="De-Occlusion"></a>De-Occlusion</h2><p>[Robust LSTM-Autoencoders for Face De-Occlusion in the Wild] <a href="https://arxiv.org/abs/1612.08534" target="_blank" rel="noopener">[Paper]</a></p>
<h2 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h2><p>[Adversarial Deep Structural Networks for Mammographic Mass Segmentation] <a href="https://arxiv.org/abs/1612.05970" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/wentaozhu/adversarial-deep-structural-networks" target="_blank" rel="noopener">[Code]</a></p>
<p>[Semantic Segmentation using Adversarial Networks] <a href="https://arxiv.org/abs/1611.08408" target="_blank" rel="noopener">[Paper]</a>（soumith’s paper）</p>
<h2 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h2><p>[Perceptual generative adversarial networks for small object detection] <a href="https://arxiv.org/abs/1706.05274v2" target="_blank" rel="noopener">[Paper]</a>(CVPR 2017)</p>
<p>[A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection] <a href="http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/xiaolonw/adversarial-frcnn" target="_blank" rel="noopener">[code]</a>(CVPR2017)</p>
<h2 id="Landmark-Detection"><a href="#Landmark-Detection" class="headerlink" title="Landmark Detection"></a>Landmark Detection</h2><p>[Style aggregated network for facial landmark detection] <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Style_Aggregated_Network_CVPR_2018_paper.pdf" target="_blank" rel="noopener">[Paper]</a>(CVPR 2018)</p>
<h2 id="Conditional-Adversarial"><a href="#Conditional-Adversarial" class="headerlink" title="Conditional Adversarial"></a>Conditional Adversarial</h2><p>[Conditional Generative Adversarial Nets] <a href="https://arxiv.org/abs/1411.1784" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/zhangqianhui/Conditional-Gans" target="_blank" rel="noopener">[Code]</a></p>
<p>[InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets] <a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/buriburisuri/supervised_infogan" target="_blank" rel="noopener">[Code]</a><a href="https://github.com/openai/InfoGAN" target="_blank" rel="noopener">[Code]</a></p>
<p>[Conditional Image Synthesis With Auxiliary Classifier GANs] <a href="https://arxiv.org/abs/1610.09585" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/buriburisuri/ac-gan" target="_blank" rel="noopener">[Code]</a>(GoogleBrain ICLR 2017)</p>
<p>[Pixel-Level Domain Transfer] <a href="https://arxiv.org/pdf/1603.07442v2.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/fxia22/pldtgan" target="_blank" rel="noopener">[Code]</a></p>
<p>[Invertible Conditional GANs for image editing] <a href="https://arxiv.org/abs/1611.06355" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/Guim3/IcGAN" target="_blank" rel="noopener">[Code]</a></p>
<p>[Plug &amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space] <a href="https://arxiv.org/abs/1612.00005v1" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/Evolving-AI-Lab/ppgn" target="_blank" rel="noopener">[Code]</a></p>
<p>[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks] <a href="https://arxiv.org/pdf/1612.03242v1.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/hanzhanggit/StackGAN" target="_blank" rel="noopener">[Code]</a></p>
<h2 id="Video-Prediction-and-Generation"><a href="#Video-Prediction-and-Generation" class="headerlink" title="Video Prediction and Generation"></a>Video Prediction and Generation</h2><p>[Deep multi-scale video prediction beyond mean square error] <a href="https://arxiv.org/abs/1511.05440" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/dyelax/Adversarial_Video_Generation" target="_blank" rel="noopener">[Code]</a>(Yann LeCun’s paper)</p>
<p>[Generating Videos with Scene Dynamics] <a href="https://arxiv.org/abs/1609.02612" target="_blank" rel="noopener">[Paper]</a><a href="http://web.mit.edu/vondrick/tinyvideo/" target="_blank" rel="noopener">[Web]</a><a href="https://github.com/cvondrick/videogan" target="_blank" rel="noopener">[Code]</a></p>
<p>[MoCoGAN: Decomposing Motion and Content for Video Generation] <a href="https://arxiv.org/abs/1707.04993" target="_blank" rel="noopener">[Paper]</a></p>
<h2 id="Texture-Synthesis-amp-style-transfer"><a href="#Texture-Synthesis-amp-style-transfer" class="headerlink" title="Texture Synthesis &amp; style transfer"></a>Texture Synthesis &amp; style transfer</h2><p>[Precomputed real-time texture synthesis with markovian generative adversarial networks] <a href="https://arxiv.org/abs/1604.04382" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/chuanli11/MGANs" target="_blank" rel="noopener">[Code]</a>(ECCV 2016)</p>
<h1 id="Image-Translation"><a href="#Image-Translation" class="headerlink" title="Image Translation"></a>Image Translation</h1><p>[UNSUPERVISED CROSS-DOMAIN IMAGE GENERATION] <a href="https://arxiv.org/abs/1611.02200" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/yunjey/domain-transfer-network" target="_blank" rel="noopener">[Code]</a></p>
<p>[Image-to-image translation using conditional adversarial nets] <a href="https://arxiv.org/pdf/1611.07004v1.pdf" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/phillipi/pix2pix" target="_blank" rel="noopener">[Code]</a><a href="https://github.com/yenchenlin/pix2pix-tensorflow" target="_blank" rel="noopener">[Code]</a></p>
<p>[Learning to Discover Cross-Domain Relations with Generative Adversarial Networks] <a href="https://arxiv.org/abs/1703.05192" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/carpedm20/DiscoGAN-pytorch" target="_blank" rel="noopener">[Code]</a></p>
<p>[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks] <a href="https://junyanz.github.io/CycleGAN/" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/junyanz/CycleGAN" target="_blank" rel="noopener">[Code]</a></p>
<p>[CoGAN: Coupled Generative Adversarial Networks] <a href="https://arxiv.org/abs/1606.07536" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/andrewliao11/CoGAN-tensorflow" target="_blank" rel="noopener">[Code]</a>(NIPS 2016)</p>
<p>[Unsupervised Image-to-Image Translation with Generative Adversarial Networks] <a href="https://arxiv.org/pdf/1701.02676.pdf" target="_blank" rel="noopener">[Paper]</a>(NIPS 2017)</p>
<p>[Unsupervised Image-to-Image Translation Networks] <a href="https://arxiv.org/abs/1703.00848" target="_blank" rel="noopener">[Paper]</a></p>
<p>[Triangle Generative Adversarial Networks] <a href="https://arxiv.org/abs/1709.06548" target="_blank" rel="noopener">[Paper]</a></p>
<p>[High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs] <a href="https://arxiv.org/abs/1711.11585" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/NVIDIA/pix2pixHD" target="_blank" rel="noopener">[code]</a></p>
<p>[XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings] <a href="https://arxiv.org/abs/1711.05139" target="_blank" rel="noopener">[Paper]</a>(Reviewed)</p>
<p>[UNIT: UNsupervised Image-to-image Translation Networks] <a href="https://arxiv.org/abs/1703.00848" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/mingyuliutw/UNIT" target="_blank" rel="noopener">[Code]</a>(NIPS 2017)</p>
<p>[Toward Multimodal Image-to-Image Translation] <a href="https://arxiv.org/abs/1711.11586" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/junyanz/BicycleGAN" target="_blank" rel="noopener">[Code]</a>(NIPS 2017)</p>
<p>[Multimodal Unsupervised Image-to-Image Translation] <a href="https://arxiv.org/abs/1804.04732" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/nvlabs/MUNIt" target="_blank" rel="noopener">[Code]</a></p>
<p>[Video-to-Video Synthesis] <a href="https://tcwang0509.github.io/vid2vid/" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/NVIDIA/vid2vid" target="_blank" rel="noopener">[Code]</a></p>
<h2 id="Facial-Attribute-Manipulation"><a href="#Facial-Attribute-Manipulation" class="headerlink" title="Facial Attribute Manipulation"></a>Facial Attribute Manipulation</h2><p>[Autoencoding beyond pixels using a learned similarity metric] <a href="https://arxiv.org/abs/1512.09300" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/andersbll/autoencoding_beyond_pixels" target="_blank" rel="noopener">[code]</a><a href="https://github.com/zhangqianhui/vae-gan-tensorflow" target="_blank" rel="noopener">[Tensorflow code]</a></p>
<p>[Coupled Generative Adversarial Networks] <a href="http://mingyuliu.net/" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/mingyuliutw/CoGAN" target="_blank" rel="noopener">[Caffe Code]</a><a href="https://github.com/andrewliao11/CoGAN-tensorflow" target="_blank" rel="noopener">[Tensorflow Code]</a>（NIPS）</p>
<p>[Invertible Conditional GANs for image editing] <a href="https://drive.google.com/file/d/0B48XS5sLi1OlRkRIbkZWUmdoQmM/view" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/Guim3/IcGAN" target="_blank" rel="noopener">[Code]</a></p>
<p>[Learning Residual Images for Face Attribute Manipulation] <a href="https://arxiv.org/abs/1612.05363" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/Zhongdao/FaceAttributeManipulation" target="_blank" rel="noopener">[code]</a>(CVPR 2017)</p>
<p>[Neural Photo Editing with Introspective Adversarial Networks] <a href="https://arxiv.org/abs/1609.07093" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/ajbrock/Neural-Photo-Editor" target="_blank" rel="noopener">[Code]</a>(ICLR 2017)</p>
<p>[Neural Face Editing with Intrinsic Image Disentangling] <a href="https://arxiv.org/abs/1704.04131" target="_blank" rel="noopener">[Paper]</a>(CVPR 2017)</p>
<p>[GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Data ] <a href="https://arxiv.org/abs/1705.04932" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/Prinsphield/GeneGAN" target="_blank" rel="noopener">[code]</a>(BMVC 2017)</p>
<p>[ST-GAN: Unsupervised Facial Image Semantic Transformation Using Generative Adversarial Networks] <a href="http://proceedings.mlr.press/v77/zhang17c.html" target="_blank" rel="noopener">[Paper]</a></p>
<p>[Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis] <a href="https://arxiv.org/abs/1704.04086" target="_blank" rel="noopener">[Paper]</a>(ICCV 2017)</p>
<p>[StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation] <a href="https://arxiv.org/abs/1711.09020" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/yunjey/StarGAN" target="_blank" rel="noopener">[code]</a>(CVPR 2018)</p>
<p>[Arbitrary Facial Attribute Editing: Only Change What You Want] <a href="https://arxiv.org/abs/1711.10678" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/LynnHo/AttGAN-Tensorflow" target="_blank" rel="noopener">[code]</a></p>
<p>[ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes] <a href="https://arxiv.org/abs/1803.10562" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/Prinsphield/ELEGANT" target="_blank" rel="noopener">[code]</a>(ECCV 2018)</p>
<p>[Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation] <a href="https://arxiv.org/abs/1805.07509" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/zhangqianhui/Sparsely-Grouped-GAN" target="_blank" rel="noopener">[code]</a>(ACM MM2018 oral)</p>
<p>[GANimation: Anatomically-aware Facial Animation from a Single Image] <a href="http://www.albertpumarola.com/research/GANimation/index.html" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/albertpumarola/GANimation" target="_blank" rel="noopener">[code]</a>(ECCV 2018 oral)</p>
<h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><p>[Connecting Generative Adversarial Networks and Actor-Critic Methods] <a href="https://arxiv.org/abs/1610.01945" target="_blank" rel="noopener">[Paper]</a>(NIPS 2016 workshop)</p>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>[C-RNN-GAN: Continuous recurrent neural networks with adversarial training] <a href="https://arxiv.org/abs/1611.09904" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/olofmogren/c-rnn-gan" target="_blank" rel="noopener">[Code]</a><br>[SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient] <a href="https://arxiv.org/abs/1609.05473" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/LantaoYu/SeqGAN" target="_blank" rel="noopener">[Code]</a>(AAAI 2017)</p>
<h1 id="Medicine"><a href="#Medicine" class="headerlink" title="Medicine"></a>Medicine</h1><p>[Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery] <a href="https://arxiv.org/abs/1703.05921" target="_blank" rel="noopener">[Paper]</a></p>
<h2 id="3D"><a href="#3D" class="headerlink" title="3D"></a>3D</h2><p>[Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling] <a href="https://arxiv.org/abs/1610.07584" target="_blank" rel="noopener">[Paper]</a><a href="http://3dgan.csail.mit.edu/" target="_blank" rel="noopener">[Web]</a><a href="https://github.com/zck119/3dgan-release" target="_blank" rel="noopener">[code]</a>(2016 NIPS)</p>
<p>[Transformation-Grounded Image Generation Network for Novel 3D View Synthesis] <a href="http://www.cs.unc.edu/%7Eeunbyung/tvsn/" target="_blank" rel="noopener">[Web]</a>(CVPR 2017)</p>
<h2 id="MUSIC"><a href="#MUSIC" class="headerlink" title="MUSIC"></a>MUSIC</h2><p>[MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation using 1D and 2D Conditions] <a href="https://arxiv.org/abs/1703.10847" target="_blank" rel="noopener">[Paper]</a><a href="https://richardyang40148.github.io/TheBlog/midinet_arxiv_demo.html" target="_blank" rel="noopener">[HOMEPAGE]</a></p>
<h2 id="For-discrete-distributions"><a href="#For-discrete-distributions" class="headerlink" title="For discrete distributions"></a>For discrete distributions</h2><p>[Maximum-Likelihood Augmented Discrete Generative Adversarial Networks] <a href="https://arxiv.org/abs/1702.07983v1" target="_blank" rel="noopener">[Paper]</a></p>
<p>[Boundary-Seeking Generative Adversarial Networks] <a href="https://arxiv.org/abs/1702.08431" target="_blank" rel="noopener">[Paper]</a></p>
<p>[GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution] <a href="https://arxiv.org/abs/1611.04051" target="_blank" rel="noopener">[Paper]</a></p>
<h2 id="Improving-Classification-And-Recong"><a href="#Improving-Classification-And-Recong" class="headerlink" title="Improving Classification And Recong"></a>Improving Classification And Recong</h2><p>[Generative OpenMax for Multi-Class Open Set Classification] <a href="https://arxiv.org/pdf/1707.07418.pdf" target="_blank" rel="noopener">[Paper]</a>(BMVC 2017)</p>
<p>[Controllable Invariance through Adversarial Feature Learning] <a href="https://arxiv.org/abs/1705.11122" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/github-pengge/adversarial_invariance_feature_learning" target="_blank" rel="noopener">[code]</a>(NIPS 2017)</p>
<p>[Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro] <a href="https://arxiv.org/abs/1701.07717" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/layumi/Person-reID_GAN" target="_blank" rel="noopener">[Code]</a> (ICCV2017)</p>
<p>[Learning from Simulated and Unsupervised Images through Adversarial Training] <a href="https://arxiv.org/abs/1612.07828" target="_blank" rel="noopener">[Paper]</a><a href="https://github.com/carpedm20/simulated-unsupervised-tensorflow" target="_blank" rel="noopener">[code]</a>（Apple paper, CVPR 2017 Best Paper）</p>
<h1 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h1><p>[cleverhans] <a href="https://github.com/openai/cleverhans" target="_blank" rel="noopener">[Code]</a>(A library for benchmarking vulnerability to adversarial examples)</p>
<p>[reset-cppn-gan-tensorflow] <a href="https://github.com/hardmaru/resnet-cppn-gan-tensorflow" target="_blank" rel="noopener">[Code]</a>(Using Residual Generative Adversarial Networks and Variational Auto-encoder techniques to produce high resolution images)</p>
<p>[HyperGAN] <a href="https://github.com/255bits/HyperGAN" target="_blank" rel="noopener">[Code]</a>(Open source GAN focused on scale and usability)</p>
<h1 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h1><table>
<thead>
<tr>
<th style="text-align:center">Author</th>
<th style="text-align:center">Address</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>inFERENCe</strong></td>
<td style="text-align:center"><a href="http://www.inference.vc/" target="_blank" rel="noopener">Adversarial network</a></td>
</tr>
<tr>
<td style="text-align:center"><strong>inFERENCe</strong></td>
<td style="text-align:center"><a href="http://www.inference.vc/infogan-variational-bound-on-mutual-information-twice/" target="_blank" rel="noopener">InfoGan</a></td>
</tr>
<tr>
<td style="text-align:center"><strong>distill</strong></td>
<td style="text-align:center"><a href="http://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">Deconvolution and Image Generation</a></td>
</tr>
<tr>
<td style="text-align:center"><strong>yingzhenli</strong></td>
<td style="text-align:center"><a href="http://www.yingzhenli.net/home/blog/?p=421http://www.yingzhenli.net/home/blog/?p=421" target="_blank" rel="noopener">Gan theory</a></td>
</tr>
<tr>
<td style="text-align:center"><strong>OpenAI</strong></td>
<td style="text-align:center"><a href="https://openai.com/blog/generative-models/" target="_blank" rel="noopener">Generative model</a></td>
</tr>
</tbody>
</table>
<h1 id="Tutorial"><a href="#Tutorial" class="headerlink" title="Tutorial"></a>Tutorial</h1><p>[1] <a href="http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf" target="_blank" rel="noopener">http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf</a> (NIPS Goodfellow Slides)<a href="http://c.m.163.com/news/a/C7UE2MLT0511AQHO.html?spss=newsapp&amp;spsw=1" target="_blank" rel="noopener">[Chinese Trans]</a><a href="https://arxiv.org/pdf/1701.00160v1.pdf" target="_blank" rel="noopener">[details]</a></p>
<p>[2] <a href="https://drive.google.com/file/d/0BxKBnD5y2M8NbzBUbXRwUDBZOVU/view" target="_blank" rel="noopener">[PDF]</a>(NIPS Lecun Slides)</p>
<p>[3] <a href="https://sites.google.com/view/iccv-2017-gans/schedule" target="_blank" rel="noopener">[ICCV 2017 Tutorial About GANS]</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/生成对抗网络/" rel="tag"># 生成对抗网络</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/17/Very-Deep-Convolution-Networks-For-Large-Scale-Image-Recognition/" rel="next" title="Very Deep Convolution Networks For Large-Scale Image Recognition">
                <i class="fa fa-chevron-left"></i> Very Deep Convolution Networks For Large-Scale Image Recognition
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/24/DRL-paper/" rel="prev" title="DRL paper">
                DRL paper <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">wangxiaxia</p>
              <p class="site-description motion-element" itemprop="description">滴水穿石</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#First-paper"><span class="nav-number">1.</span> <span class="nav-text">First paper</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unclassified"><span class="nav-number">2.</span> <span class="nav-text">Unclassified</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GAN-Theory"><span class="nav-number">3.</span> <span class="nav-text">GAN Theory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generation-High-Quality-Images"><span class="nav-number">4.</span> <span class="nav-text">Generation High-Quality Images</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Semi-Supervised-Learning"><span class="nav-number">5.</span> <span class="nav-text">Semi-Supervised Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ensemble"><span class="nav-number">6.</span> <span class="nav-text">Ensemble</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Image-blending"><span class="nav-number">7.</span> <span class="nav-text">Image blending</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Image-Inpainting"><span class="nav-number">8.</span> <span class="nav-text">Image Inpainting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Re-identification"><span class="nav-number">9.</span> <span class="nav-text">Re-identification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Super-Resolution"><span class="nav-number">10.</span> <span class="nav-text">Super-Resolution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#De-Occlusion"><span class="nav-number">11.</span> <span class="nav-text">De-Occlusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Semantic-Segmentation"><span class="nav-number">12.</span> <span class="nav-text">Semantic Segmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Object-Detection"><span class="nav-number">13.</span> <span class="nav-text">Object Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Landmark-Detection"><span class="nav-number">14.</span> <span class="nav-text">Landmark Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conditional-Adversarial"><span class="nav-number">15.</span> <span class="nav-text">Conditional Adversarial</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Video-Prediction-and-Generation"><span class="nav-number">16.</span> <span class="nav-text">Video Prediction and Generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Texture-Synthesis-amp-style-transfer"><span class="nav-number">17.</span> <span class="nav-text">Texture Synthesis &amp; style transfer</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Image-Translation"><span class="nav-number"></span> <span class="nav-text">Image Translation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Facial-Attribute-Manipulation"><span class="nav-number">1.</span> <span class="nav-text">Facial Attribute Manipulation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reinforcement-learning"><span class="nav-number">2.</span> <span class="nav-text">Reinforcement learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN"><span class="nav-number">3.</span> <span class="nav-text">RNN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Medicine"><span class="nav-number"></span> <span class="nav-text">Medicine</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3D"><span class="nav-number">1.</span> <span class="nav-text">3D</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MUSIC"><span class="nav-number">2.</span> <span class="nav-text">MUSIC</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#For-discrete-distributions"><span class="nav-number">3.</span> <span class="nav-text">For discrete distributions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improving-Classification-And-Recong"><span class="nav-number">4.</span> <span class="nav-text">Improving Classification And Recong</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Project"><span class="nav-number"></span> <span class="nav-text">Project</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Blogs"><span class="nav-number"></span> <span class="nav-text">Blogs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tutorial"><span class="nav-number"></span> <span class="nav-text">Tutorial</span></a></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wangxiaxia</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
